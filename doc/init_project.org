#+startup:fold
* init_project.py                                                            

  _init_project.py_ is present in each directory corrresponding to a dataset family
  It defines the configuration of all grid jobs for this family

- a Mu2e job is defined by its input dataset and the configuration (FHICL) file

- generation may consists of several stages, each stage may have several jobs, for example 

  - stage 1:
    - job 1: generation of proton interaction in the production target and tracing the beam down to TS3
    - job 2: concatenation of the job 1 output
    - job 3: ntupling of the stage 1 output

- a Mu2e job is therefore defined by its input dataset and the configuration (FHICL) file

  For each dataset family, all grid jobs for all stages are defined 
  in a file _$project/datasets/$family/init_project.py_ 

- definition of the input dataset:

 job.fInputDataset = Dataset(defname,dsid,location)

  parameters:

  - defname  : full dataset definition name, i.e. 'dig.mu2e.bmum1s52b0.pbar2m.art'
    - defname='generator' : dataset has no input files, the job generates events rather than 
      reads them from an input file
  - dsid     : short dataset definition, still uniquely identifying the dataset
  - location : location of the dataset catalog:
    - 'local' : the dataset catalog resides in $project/datasets/$family/catalog/$defname.files
    - 'sam'   : the dataset catalog resides in SAM 

  example: 
#+begin_src 
job.fInputDataset = Dataset('dig.mu2e.bmum1s52b0.pbar2m.art','bmum1s52b0','local')
#+end_src 
* FCL file names                                                             
  for each job, the FCL file name is constructed as follows: 

            ${stage_name}_${job_name}_${project_name}.fcl

* defining a stage                                                           
  - stage has just a name
  - stage name is embedded into the name of FCL file
  - for a multistage job, it is convenient to embed a number into the stage name - then 
    all config files in the directory catalog get ordered naturally 
#+begin_src 
  s = self.new_stage('s3');
#+end_src

* parameters defining a job                                                  
- *job.fNInputFiles*                                                         
  - if the 'input dataset' is a generator, this parameter defines 
                       the number of segments in the submitted job
  - for a regular input dataset, set to -1: in this case, the number of input 
    segments is calculated based on the number of files in the input dataset 
    and the value of *job.fMaxInputFilesPerSegment*
- *job.fMaxInputFilesPerSegment*                                             
  - max number of input files per grid job segment
- *job.fIfdh*                                                                
  - file delivery protocol, either 'xrootd' or 'ifdh'
- *job.fMaxMemory*                                                           
  - max memory per segment, passed to teh grid job submitter
- *job.fOutputStream*                                                        
  - names of the job output streams (array)
- *job.fOutputDsID*                                                          
  - for each output stream, defined the output dataset ID's
- *job.fOutputPath*                                                          
  - for each output stream, the FCL path corresponding to it
- *job.fOutputFnPattern*                                                     
  - for each output stream, defines the name the output file 
- *job.fOutputFormat*                                                        
  - for each output stream, defines the output file format ('art', 'stn')
- *job.fDescription*                                                         
  - name stub fo the grid output directory
- *job.fRequestedTime*                                                       
  - requested time per segment, in hours. Depending on the grid node, 
    the job time can vary by a factor of ~3. 
* ------------------------------------------------------------------------------
* back to file:workflow.org
* ------------------------------------------------------------------------------

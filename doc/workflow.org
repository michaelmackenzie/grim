#+startup:fold
# ------------------------------------------------------------------------------
* 0.0 
  - each dataset family is described in a separate subdirectory: $project/datasets/$family
  - for a given family, configuration of all jobs is specified in $project/datasets/$family/init_project.py file.
  - datasets catalogs could be stored locally or in the DH system (SAM)
  - local catalogs are kept in the directory named $project/datasets/$family/catalog
* [[file:init_project.org][init_project.py]] : definition of the workflow
* ------------------------------------------------------------------------------
* steps of the workflow
* ------------------------------------------------------------------------------
* 0.1) clone offline code from git, compile it
* 0.2) create _./.grid_config_ configuration file                            

#+begin_src 
  source grim/scripts/create_project pbar2m   # project=pbar2m
#+end_src

  - it is created in the working (muse) directory of your project.
  - Below, $project stands for the project name
  - a .grid_config file could specify several projects 

   the template file:../scripts/.grid_config specifies just one project. 

   Quick explanation of the configuration parameters:

- $project.base_release     : test release, the area where you build your code in _build_ subdirectory
- $project.code_tarball_dir : location of the code tarball, if not specified: /mu2e/data/users/$USER/grid
                              build_tarball.py puts it there for job_submit.py to pick up
- $project.code_taball      : name of the code tarball to use
- $project.tmp_dir          : area used to keep the project book-keeping information 
- $project.grid_output_dir  : the grid output arrives to project.grid_output_dir.$USER/workflow
- $project.log_dir          : area used to store the log files (away from /pnfs)

  - edit the .grid_config file if needed - everything there should be self-explanatory 
    for anyone who ever submitted a grid job

  To add another project, do the same - edit the *.grid_config* file manually
  
  Note, that the code tarball tag has to be specified manually after the tarball 
  has been built (see next step).

* 0.3) build the code tarball                                                
  
   grim/scripts/build_tarball.py --project=$project

   the code tarball is automatically copied to /mu2e/data/users/$USER/grid/$project
   after the tarball is built, update .grid_config file to specify the tarball name there
   It is needed for grim/scripts/submit_job.py to find the code tarball 

   note: if /mu2e/data/users/$USER/grid/$project directory doesnt exist, just create it
   
   if the tarball is built under personal account and you're running as mu2epro, 
   copy tarball (as mu2epro), do teh following:

   grim/scripts/copy_user_tarball tarball

   Comment: using optimized libraries for grid submission saves a factor of x2-3
   in execution time.
	  
* 1.0) generate the FCL tarball                                              

   grim/scripts/gen_fcl.py .. parameters... (see gen_fcl help below)

   to generate the fcl tarfile for grid submission

   The fcl tarball is also copied to /pnfs/mu2e/scratch/$USER/fcl/$project 
   for jobsub to find it there

   again, if the directory doesn't exist, create it manually.
   note that 

* 1.1) test the tarballs locally                                             

- create a new directory, /mu2e/data/users/$USER/grid could be a good name
- untar the code tarball in a new shell to there
- source Code/setup.sh
- run your executable. For testing, you could use one of the FCL's created
  by generate_fcl - find it in the tmp/$project/fcl/ subdirectory
  of your working area.

* 2.0) submit the grid job                                                   
   
#+begin_src                       
   grim/scripts/submit_job.py  [parameters]
#+end_src
   to submit a grid job. Parameters are described 

* 3.0) monitor status of submitted job(s) by running _grid_monitor.py_       

  grim/scripts/grid_monitor.py --project=$project

  When a job finishes, *grid_monitor* changes status of the job from 'running' to 'finished'

* 4.0) run _check_completed_job.py_ to identify failed segments :            

  grim/scripts/check_completed_job.py --project=<project> --grid_id=xxxxxxxx

  the script will check if all job segments have completed successfully and, 
  if some have failed, will automatically create an input for the recovery job. 
  To create an fcl tarball for the recovery job, run

  grim/scripts/gen_fcl.py --project=<project> --recover=<grid_id>
   
  where <grid_id> is a GRID ID of the initial job which needs to be recovered. 

* 4.1) if there were failed segments, try to recover, if needed              

  a) generate an FCL tarball for the recovery job 

  grim/scripts/gen_fcl.py --project=<project> --recover=<grid_id>

  b) submit a recovery job                  

  grim/scripts/submit_job.py --project=<project> --recover=<grid_id>

* 4.2) create input for the next stage                                       

  grim/scripts/list_pnfs_files.py --project=<project> --grid_id=<grid_id>

* 4.3) save log files of a successfully completed job:                       

       grim/scripts/copy_log_files.py --grid_id=<grid_id>

      do all that only after running grid_monitor.py - the scripts operate only on completed jobs, 
      which status files are available in tmp/$project/completed_jobs

* 5.0) proceed with the next stage
* ------------------------------------------------------------------------------
* pileup generation                                                          
- datasets describing additional inputs to generate the pileup are project-dependent.
  They are expected to be defined in the project configuration - 

                  $project/datasets/mixing/mixing_inputs.py 

* ------------------------------------------------------------------------------
* back to file:grim.org
* ------------------------------------------------------------------------------
